{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "001e4cec",
   "metadata": {},
   "source": [
    "**1.\tWhat are Vanilla autoencoders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e91988",
   "metadata": {},
   "source": [
    "1. The vanilla autoencoder consists of only one hidden layer. The number of neurons in the hidden layer is less than the number of neurons in the input (or output) layer. This results in producing a bottleneck effect on the flow of information in the network, and therefore we can think of the hidden layer as a bottleneck layer, restricting the information that would be stored.\n",
    "2. Learning in the autoencoder consists of developing a compact representation of the input signal at the hidden layer so that the output layer can faithfully reproduce the original input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6ee070",
   "metadata": {},
   "source": [
    "**2.\tWhat are Sparse autoencoders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b02a36",
   "metadata": {},
   "source": [
    "1. A sparse autoencoder is simply an autoencoder whose training criterion involves a sparsity penalty. In most cases, we would construct our loss function by penalizing activations of hidden layers so that only a few nodes are encouraged to activate when a single sample is fed into the network.\n",
    "2. There are actually two different ways to construct our sparsity penalty: L1 regularization and KL-divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960bfde",
   "metadata": {},
   "source": [
    "**3.\tWhat are Denoising autoencoders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfe0749",
   "metadata": {},
   "source": [
    "![](https://929687.smushcdn.com/2407837/wp-content/uploads/2020/02/keras_denoising_autoencoder_overview.png?lossy=1&strip=1&webp=1)\n",
    "\n",
    "1. Denoising autoencoders are an extension of simple autoencoders. However, it’s worth noting that denoising autoencoders were not originally meant to automatically denoise an image.\n",
    "2. Denoising autoencoder procedure was invented to help:\n",
    "    1. The hidden layers of the autoencoder learn more robust filters\n",
    "    2. Reduce the risk of overfitting in the autoencoder\n",
    "    3. Prevent the autoencoder from learning a simple identify function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a2b7e",
   "metadata": {},
   "source": [
    "**4.\tWhat are Convolutional autoencoders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01d4d0",
   "metadata": {},
   "source": [
    "1. Convolutional Autoencoder is a variant of Convolutional Neural Networks that are used as the tools for unsupervised learning of convolution filters. They are generally applied in the task of image reconstruction to minimize reconstruction errors by learning the optimal filters.\n",
    "2. Once they are trained in this task, they can be applied to any input in order to extract features. Convolutional Autoencoders are general-purpose feature extractors differently from general autoencoders that completely ignore the 2D image structure.\n",
    "3. In autoencoders, the image must be unrolled into a single vector and the network must be built following the constraint on the number of inputs.\n",
    "\n",
    "![](https://149695847.v2.pressablecdn.com/wp-content/uploads/2020/07/The-structure-of-proposed-Convolutional-AutoEncoders-CAE-for-MNIST-In-the-middle-there.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258536a5",
   "metadata": {},
   "source": [
    "**5.\tWhat are Stacked autoencoders**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92433451",
   "metadata": {},
   "source": [
    "1. A stacked autoencoder is a neural network consist several layers of sparse autoencoders where output of each hidden layer is connected to the input of the successive hidden layer.\n",
    "2. Stacked autoencoder improving accuracy in deep learning with noisy autoencoders embedded in the layers\n",
    "3. Steps that are need to be implemented are:\n",
    "    1. Train autoencoder using input data and acquire the learned data.\n",
    "    2. The learned data from the previous layer is used as an input for the next layer and this continues until the training is completed.\n",
    "    3. Once all the hidden layers are trained use the backpropagation algorithm to minimize the cost function and weights are updated with the training set to achieve fine tuning.\n",
    "![](https://miro.medium.com/max/1400/1*7H9VQlN94-wv7Ianqt6GZg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3ba6c",
   "metadata": {},
   "source": [
    "**6.\tExplain how to generate sentences using LSTM autoencoders**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2b7ff",
   "metadata": {},
   "source": [
    "1. An LSTM Autoencoder is an implementation of an autoencoder for sequence data using an Encoder-Decoder LSTM architecture.\n",
    "2. In the sentence generation, we try to predict the next character or word of the sequence. The text data generally considered as sequence of data. \n",
    "3. For predicting data in sequence we used deep learning models like RNN or LSTM. LSTM are preferred over RNN in this because of RNN vanishing and exploding gradients problem. Since in text generation we have to memorize large amount of previous data. So for this purpose LSTM are preferred.\n",
    "4. The steps performed are: \n",
    "    1. Load the necessary libraries required for LSTM and NLP purposes\n",
    "    2. Load the text data\n",
    "    3. Performing the required text cleaning\n",
    "    4. Create a dictionary of words with keys as integer values\n",
    "    5. Prepare dataset as input and output sets using dictionary\n",
    "    6. Define our LSTM model for text generation\n",
    "\n",
    "![](https://iq.opengenus.org/content/images/2019/12/1_XvUt5wDQA8D3C0wAuxAvbA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754b740",
   "metadata": {},
   "source": [
    "**7.\tExplain Extractive summarization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7826c5a",
   "metadata": {},
   "source": [
    "1. Automatic text summarization is the task of producing a concise and fluent summary while preserving key information content and overall meaning\n",
    "2. In Extractive Summarization, we are identifying important phrases or sentences from the original text and extract only these phrases from the text. These extracted sentences would be the summary.\n",
    "![](https://lh3.googleusercontent.com/trCR1Oav-Io888npW16rRzcB6hRMcXJrJemLK2znK9mC5ODoUX7NG9Sjn45JXzLEl8ERm4QoGHpwBxntFVzhJ4ITraVsYogslm1_TR_KkvdmSiINpVxr-uBGBbNcK_EI6OuRFfU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d5c86e",
   "metadata": {},
   "source": [
    "**8.\tExplain Abstractive summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df054e9",
   "metadata": {},
   "source": [
    "1. In the Abstractive Summarization approach, we work on generating new sentences from the original text. The abstractive method is in contrast to the approach that was described above. The sentences generated through this approach might not even be present in the original text.\n",
    "2. We are going to focus on using extractive methods. This method functions by identifying important sentences or excerpts from the text and reproducing them as part of the summary. In this approach, no new text is generated, only the existing text is used in the summarization process. \n",
    "\n",
    "![](https://lh5.googleusercontent.com/2I_KpeN5xafR4dvnqGAn4U2L8zu2Ih9BmiJ5V741dz3zxjcLPwomA_RUEvnfk_ebtNV69yC0zDHC5MjooNAX_0cwarW8B6fayAWth_Yp02q8rpErUATP7KDT2DAU40DsbBYgDYs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52254e9f",
   "metadata": {},
   "source": [
    "**9.\tExplain Beam search**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04492c4f",
   "metadata": {},
   "source": [
    "1. The beam search algorithm selects multiple alternatives for an input sequence at each timestep based on conditional probability. The number of multiple alternatives depends on a parameter called Beam Width B. At each time step, the beam search selects B number of best alternatives with the highest probability as the most likely possible choices for the time step.\n",
    "2. A higher beam width will give a better translation but would use a lot of memory and computational power.\n",
    "3. A lower beam width will result in more inferior quality translation but will be fast and efficient in terms of memory usage and computational power\n",
    "4. The steps involved are:\n",
    "    1. Find the top 3 words with the highest probability given the input sentence.\n",
    "    2. Find the three best pairs for the first and second words based on conditional probability\n",
    "    3. Find the three best pairs for the first, second and third word based on the input sentence and the chosen first and the second word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a8a714",
   "metadata": {},
   "source": [
    "**10.\tExplain Length normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b63cb8",
   "metadata": {},
   "source": [
    "1. Length normalization is a small change to the beam search algorithm that can help get much better results.\n",
    "2. Beam search is maximizing the probability in the first formula below. It is the product of all the probabilities where t is total number of words in the output.\n",
    "3. Since these probabilities are all numbers less than 1, multiplying a lot of numbers less than 1 will result in a tiny, tiny, tiny number, which can result in numerical underflow. So in practice, instead of maximizing this product, we will take logs and log of a product becomes sum of a log which is the 2nd formula. This provides us with a numerically stable algorithm that is less prone to rounding errors. Because the log function is strictly monotonically increasing function, we know that maximizing log P(y) given x should give the same result as maximizing P(y) given x.\n",
    "4. This objective function has an undesirable effect where it unnaturally tends to prefer very short translations(because the value of multiplying small number of probabilities is higher). The same is true for the log of probability (since the log of values less than 1 is in negative range). To solve this, we normalize the result by dividing it by number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4037db7",
   "metadata": {},
   "source": [
    "**11.\tExplain Coverage normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c017c",
   "metadata": {},
   "source": [
    "1. Scores are penalized by the following formula \n",
    "2. Where β is the coverage penalty\n",
    "3. where pi,j is the attention probability of the j-th target word yj on the i-th source word xi, |X| is the source length, |Y| is the current target length and β is the coverage normalization coefficient\n",
    "![](https://blog.ceshine.net/post/implementing-beam-search-part-2/1*EvpXElfNvVHyPo8_ZZRong.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a235c641",
   "metadata": {},
   "source": [
    "**12.\tExplain ROUGE metric evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e471766",
   "metadata": {},
   "source": [
    "1. ROUGE means Recall-Oriented Understudy for Gisting Evaluation.\n",
    "2. ROUGE-N measures the number of matching ‘n-grams’ between our model-generated text and a ‘reference’.\n",
    "3. An n-gram is simply a grouping of tokens/words. A unigram (1-gram) would consist of a single word. A bigram (2-gram) consists of two consecutive words\n",
    "4. Recall, Precision, F1-Score, ROUGE-L, LCS, ROUGE-S are the metrics which are under ROUGE metric evaluation.\n",
    "\n",
    "Precision: \n",
    "\n",
    "![](https://miro.medium.com/max/875/1*aSd89F6kupr3znW71Qmb3Q.png)\n",
    "\n",
    "F1-Score:\n",
    "![](https://miro.medium.com/max/875/1*zYuwaCDNpYf51H5S4DpDRA.png)\n",
    "\n",
    "Rouge-L: ROUGE-L measures the longest common subsequence (LCS) between our model output and reference. All this means is that we count the longest sequence of tokens that is shared between both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f233d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
