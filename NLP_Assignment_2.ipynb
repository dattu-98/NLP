{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aca3720",
   "metadata": {},
   "source": [
    "**1.\tWhat are Corpora?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e98838",
   "metadata": {},
   "source": [
    "1. Corpara is a large and structured set of machine-readable texts that have been produced in a natural communicative setting.\n",
    "2. They can be derived in different ways like text that was originally electronic, transcripts of spoken language and optical character recognition.\n",
    "3. A good Corpora should have good Depth, Recent text,Metadata, Genre, Size, Clean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8832ad3d",
   "metadata": {},
   "source": [
    "**2.\tWhat are Tokens?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3934c",
   "metadata": {},
   "source": [
    "1. Tokenization is a common task in Natural Language Processing (NLP). It’s a fundamental step in both traditional NLP methods like Count Vectorizer and Advanced Deep Learning-based architectures like Transformers. \n",
    "2. Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types – word, character, and subword (n-gram characters) tokenization.\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/rnn.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a1dd3c",
   "metadata": {},
   "source": [
    "**3.\tWhat are Unigrams, Bigrams, Trigrams?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a296d3",
   "metadata": {},
   "source": [
    "1. An N-gram is a sequence of N tokens (or words).\n",
    "2. A 1-gram (or unigram) is a one-word sequence. \n",
    "3. A 2-gram (or Bigram) is a two-word sequence. \n",
    "4. A 3-gram (or Trigram) is a three-word sequence. \n",
    "\n",
    "![](https://www.oreilly.com/library/view/artificial-intelligence-for/9781788472173/assets/447da4ba-e40a-4787-a2f2-20a906077475.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd29d13",
   "metadata": {},
   "source": [
    "**4.\tHow to generate n-grams from text?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c5d7ca",
   "metadata": {},
   "source": [
    "Steps Implemented for N-gram in nlp\n",
    "1. Explore the dataset\n",
    "2. Feature extraction\n",
    "3. Train-test split\n",
    "4. Basic pre-processing\n",
    "5. Code to generate N-grams\n",
    "6. Creating unigrams\n",
    "7. Creating bigrams\n",
    "8. Creating trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b112ab1",
   "metadata": {},
   "source": [
    "**5.\tExplain Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310dc700",
   "metadata": {},
   "source": [
    "1. Lemmatization is one of the most common text pre-processing techniques used in Natural Language Processing (NLP) and machine learning in general.\n",
    "2. In lemmatization, on the other hand, the algorithms have this knowledge. In fact, you can even say that these algorithms refer a dictionary to understand the meaning of the word before reducing it to its root word, or lemma.\n",
    "3. A lemmatization algorithm would know that the word \"better\" is derived from the word \"good\" and hence, the lemme is good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67797c6",
   "metadata": {},
   "source": [
    "**6.\tExplain Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc2b608",
   "metadata": {},
   "source": [
    "1. Stemming is a natural language processing technique that lowers inflection in words to their root forms, hence aiding in the preprocessing of text, words, and documents for text normalization. \n",
    "2. In stemming, a part of the word is just chopped off at the tail end to arrive at the stem of the word. There are definitely different algorithms used to find out how many characters have to be chopped off, but the algorithms don’t actually know the meaning of the word in the language it belongs to.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*ES5bt7IoInIq2YioQp2zcQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062bdffc",
   "metadata": {},
   "source": [
    "**7.\tExplain Part-of-speech (POS) tagging**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af9ab16",
   "metadata": {},
   "source": [
    "1. Part-of-speech (POS) tagging is a popular Natural Language Processing process which refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context.\n",
    "2. Part-of-speech tags describe the characteristic structure of lexical terms within a sentence or text, therefore, we can use them for making assumptions about semantics.\n",
    "3. Applications are Named Entity Recognition, Co-reference resolution, Speech recognition\n",
    "\n",
    "![](https://cdn-media-1.freecodecamp.org/images/1*f6e0uf5PX17pTceYU4rbCA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151bb69",
   "metadata": {},
   "source": [
    "**8.\tExplain Chunking or shallow parsing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d7f92",
   "metadata": {},
   "source": [
    "1. Chunking is defined as the process of natural language processing used to identify parts of speech and short phrases present in a given sentence.\n",
    "2. Chunking is used to get the required phrases from a given sentence. However, POS tagging can be used only to spot the parts of speech that every word of the sentence belongs to.\n",
    "3. There are two types of chunking. Chunking up and chunking down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef78161",
   "metadata": {},
   "source": [
    "**9.\tExplain Noun Phrase (NP) chunking**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79b0e3a",
   "metadata": {},
   "source": [
    "1. Text chunking is dividing sentences into non-overlapping phrases.\n",
    "2. Noun phrase chunking deals with extracting the noun phrases from a sentence. While NP chunking is much simpler than parsing, it is still a challenging task to build a accurate and very efficient NP chunker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d9a1d",
   "metadata": {},
   "source": [
    "**10.\tExplain Named Entity Recognition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e249641",
   "metadata": {},
   "source": [
    "1. Named entity recognition is a natural language processing technique that can automatically scan entire articles and pull out some fundamental entities in a text and classify them into predefined categories.\n",
    "2. Entities can Organizations, Quantities, Monetary values, Percentages, People’s names, Company names, Geographic locations (Both physical and political),Product names, Dates and times Amounts of money, Names of events\n",
    "![](https://lh3.googleusercontent.com/9mvuuylJe5Pp_WtnH5kJjmVWz1qdLitaz16PYrZtr9DoVg33ymXAfmxdA_4eLiyTmSAU-4_iN3GyZCM-7rpLlXfu0Mla4CPKeIMZQRQufs3-QeJIXuogj67YB0pAB2I_dU2CoQ6T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e6743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
