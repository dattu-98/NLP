{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7dfd02",
   "metadata": {},
   "source": [
    "**1.\tWhat are Sequence-to-sequence models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da21200",
   "metadata": {},
   "source": [
    "1. Sequence to Sequence models is a special class of Recurrent Neural Network architectures that we typically use (but not restricted) to solve complex Language problems like Machine Translation, Question Answering, creating Chatbots, Text Summarization.\n",
    "2. So Sequence-to-sequence learning (Seq2Seq) is all about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French).\n",
    "\n",
    "![](https://miro.medium.com/max/669/0*iDgmgGnrzq65dPXy.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce715d7",
   "metadata": {},
   "source": [
    "**2.\tWhat are the Problem with Vanilla RNNs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b135baef",
   "metadata": {},
   "source": [
    "1. Recurrent neural network is a type of network architecture that accepts variable inputs and variable outputs, which contrasts with the vanilla feed-forward neural networks. We can also consider input with variable length, such as video frames and we want to make a decision along every frame of that video.\n",
    "2. Vanilla RNN has the problem of gradient exploding(W>1) or gradient vanishing(W<1) when go through many time steps (t>>1)\n",
    "3. We might loss most of the information of the first time\n",
    "4. Hidden state of a RNN cell is only passed to the next time step\n",
    "5. These limitations can be overcome from LSTM and GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c4d52d",
   "metadata": {},
   "source": [
    "**3.\tWhat is Gradient clipping?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d337bce",
   "metadata": {},
   "source": [
    "1. Gradient Clipping is a method where the error derivative is changed or clipped to a threshold during backward propagation through the network, and using the clipped gradients to update the weights. \n",
    "2. Gradient Clipping is implemented in two variants:\n",
    "    1. Clipping-by-value\n",
    "    2. Clipping-by-norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bee7ff",
   "metadata": {},
   "source": [
    "**4.\tExplain Attention mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35335781",
   "metadata": {},
   "source": [
    "1. Basically attention is the cognitive process of selectively concentrating on one or a few things while ignoring others.\n",
    "2. In the same way a neural network is considered to be an effort to mimic human brain actions in a simplified manner. Attention Mechanism is also an attempt to implement the same action of selectively concentrating on a few relevant things, while ignoring others in deep neural networks. \n",
    "3. The attention mechanism emerged as an improvement over the encoder decoder-based neural machine translation system in natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f77674",
   "metadata": {},
   "source": [
    "**5.\tExplain Conditional random fields (CRFs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5927ec37",
   "metadata": {},
   "source": [
    "1. Conditional Random Fields is a class of discriminative models best suited to prediction tasks where contextual information or state of the neighbors affect the current prediction\n",
    "2. CRFs find their applications in named entity recognition, part of speech tagging, gene prediction, noise reduction and object detection problems\n",
    "3. Since CRF is a discriminative model i.e. it models the conditional probability P(Y/X) i.e. X is always given or observed. Therefore the graph ultimately is a simple chain.\n",
    "\n",
    "![](https://miro.medium.com/max/726/1*j23aSmxJbgaTSQ3Fi0etpA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed58b9ad",
   "metadata": {},
   "source": [
    "**6.\tExplain self-attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86024616",
   "metadata": {},
   "source": [
    "1. A self-attention module takes in n inputs and returns n outputs.\n",
    "2. The self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“attention”). The outputs are aggregates of these interactions and attention scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d57645",
   "metadata": {},
   "source": [
    "**7.\tWhat is Bahdanau Attention?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115dc5b",
   "metadata": {},
   "source": [
    "1. The Bahdanau attention was proposed to address the performance bottleneck of conventional encoder-decoder architectures, achieving significant improvements over the conventional approach. \n",
    "2. The most important distinguishing feature of this approach from the basic encoder–decoder is that it does not attempt to encode a whole input sentence into a single fixed-length vector. Instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation.\n",
    "3. When predicting a token, if not all the input tokens are relevant, the RNN encoder-decoder with Bahdanau attention selectively aggregates different parts of the input sequence. This is achieved by treating the context variable as an output of additive attention pooling.\n",
    "4. In the RNN encoder-decoder, Bahdanau attention treats the decoder hidden state at the previous time step as the query, and the encoder hidden states at all the time steps as both the keys and values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac43240",
   "metadata": {},
   "source": [
    "**8.\tWhat is a Language Model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605d7ab8",
   "metadata": {},
   "source": [
    "1. A language model is basically a probability distribution over words or word sequences. In practice, a language model gives the probability of a certain word sequence being “valid”. Validity in this context does not refer to grammatical validity at all. It means that it resembles how people speak (or, to be more precise, write) — which is what the language model learns. \n",
    "2. This is extensively used in Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f3b9cc",
   "metadata": {},
   "source": [
    "**9.\tWhat is Multi-Head Attention?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ba2ed",
   "metadata": {},
   "source": [
    "1. Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently\n",
    "2. Instead of performing a single attention pooling, queries, keys, and values can be transformed with  h  independently learned linear projections. Then these  h  projected queries, keys, and values are fed into attention pooling in parallel. In the end,  h  attention pooling outputs are concatenated and transformed with another learned linear projection to produce the final output. This design is called multi-head attention, where each of the  h  attention pooling outputs is a head \n",
    "\n",
    "![](https://d2l.ai/_images/multi-head-attention.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f40461e",
   "metadata": {},
   "source": [
    "**10.\tWhat is Bilingual Evaluation Understudy (BLEU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5974fef0",
   "metadata": {},
   "source": [
    "1. BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation of text to one or more reference translations.\n",
    "2. Perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0\n",
    "3. The primary programming task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches. These matches are position-independent. The more the matches, the better the candidate translation is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e7a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
