{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8299b0",
   "metadata": {},
   "source": [
    "**1.\tExplain One-Hot Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6c627",
   "metadata": {},
   "source": [
    "One hot encoding is done to convert catrgorical variables to numerical variables. Few machine learning models can not operate with categorical data directly. So in order to convert categorical to numerical one hot encoding method is used. \n",
    "One Hot Encoding is a common way of preprocessing categorical features for machine learning models. This will improve accuracy and predictions of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e7e8c",
   "metadata": {},
   "source": [
    "**2.\tExplain Bag of Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff6516",
   "metadata": {},
   "source": [
    "Bag of Words model is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words. This is mainly used in NLP. If a word in a sentence is a frequent word, we set it as 1, else we set it as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dea0c7",
   "metadata": {},
   "source": [
    "**3.\tExplain Bag of N-Grams**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b866a3e",
   "metadata": {},
   "source": [
    "1. A bag of N-grams model represents a text document as an unordered collection of its N-grams. \n",
    "2. A bag of N-grams model has the simplicity of the bag of words model, but allows the preservation of more word locality information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc581dd",
   "metadata": {},
   "source": [
    "**4.\tExplain TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288618c0",
   "metadata": {},
   "source": [
    "TF-IDF stands for Term Frequency — Inverse Document Frequency. This is a technique to quantify words in a set of documents. We generally compute a score for each word to signify its importance in the document and corpus. This method is a widely used technique in Information Retrieval and Text Mining.\n",
    "\n",
    "![](https://plumbr.io/app/uploads/2016/06/tf-idf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c73d42",
   "metadata": {},
   "source": [
    "**5.\tWhat is OOV problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3551e3",
   "metadata": {},
   "source": [
    "1. Out-of-vocabulary (OOV) are terms that are not part of the normal lexicon found in a natural language processing environment.\n",
    "2. Out-of-vocabulary (OOV) words have higher word and sentence error rates compared to in-vocabulary (IV) words.\n",
    "3. We need to Increase vocabulary size, Use confidence scoring to detect OOV words, Use subword units in the first stage of a two-stage system, Incorporate an unknown word model into a speech recognizer\n",
    "\n",
    "\n",
    "![](https://image1.slideserve.com/3199176/answer-to-oov-problem-sub-word-std-l.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2964c605",
   "metadata": {},
   "source": [
    "**6.\tWhat are word embeddings?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1b944",
   "metadata": {},
   "source": [
    "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec77d6e",
   "metadata": {},
   "source": [
    "**7.\tExplain Continuous bag of words (CBOW)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b76f4",
   "metadata": {},
   "source": [
    "The word2vec model has two different architectures to create the word embeddings. CBOW and Skipgram model. The CBOW model tries to understand the context of the words and takes this as input. It then tries to predict words that are contextually accurate. The model tries to predict the target word by trying to understand the context of the surrounding words.\n",
    "\n",
    "![](https://149695847.v2.pressablecdn.com/wp-content/uploads/2020/09/q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eab8253",
   "metadata": {},
   "source": [
    "**8.\tExplain SkipGram**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828a8e15",
   "metadata": {},
   "source": [
    "Skip-gram is used to predict the context word for a given target word. It’s reverse of CBOW algorithm. Here, target word is input while context words are output. It requires less memory comparing with other words to vector representations\n",
    "\n",
    "![](https://miro.medium.com/max/568/1*3xy5IOpScN0aQwwfFbCmGQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6083c6",
   "metadata": {},
   "source": [
    "**9.\tExplain Glove Embeddings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a9789",
   "metadata": {},
   "source": [
    "Glove means Global Vector for word representation\n",
    "\n",
    "1. Glove is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. \n",
    "2. Glove Embeddings are a type of word embedding that encode the co-occurrence probability ratio between two words as vector differences. \n",
    "3. Embeddings use a lower-dimensional space while preserving semantic relationships.\n",
    "\n",
    "![](https://miro.medium.com/max/69/1*w5l0aWdwST4WFa414LvR2w.png)\n",
    "\n",
    "4. The ratio is better able to distinguish relevant words from irrelevant words and it is also better able to discriminate between the two relevant words.\n",
    "\n",
    "![](https://miro.medium.com/max/875/1*2HuruOHvhP7_gnW2DKB2FQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b3b5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
