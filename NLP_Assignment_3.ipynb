{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d4edf13",
   "metadata": {},
   "source": [
    "**1.\tExplain the basic architecture of RNN cell.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d899fb",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1400/1*K6s4Li0fTl1pSX4-WPBMMA.jpeg)\n",
    "\n",
    "1. The Recurrent neural networks are a class of artificial neural networks where the connection between nodes form a directed graph along a temporal sequence. Unlike the feed-forward neural networks, the recurrent neural networks use their internal state memory for processing sequences. \n",
    "2. This dynamic behavior of the Recurrent neural networks allows them to be very useful and applicable to audio analysis, handwritten recognition, and several such applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4beb62f",
   "metadata": {},
   "source": [
    "**2.\tExplain Backpropagation through time (BPTT)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b142a",
   "metadata": {},
   "source": [
    "1. Backpropagation Through Time, or BPTT, is the training algorithm used to update weights in recurrent neural networks like LSTMs.\n",
    "2. The goal of the backpropagation training algorithm is to modify the weights of a neural network in order to minimize the error of the network outputs compared to some expected output in response to corresponding inputs.\n",
    "3. The steps involved in BPTT are:\n",
    "    1. Present a training input pattern and propagate it through the network to get an output.\n",
    "    2. Compare the predicted outputs to the expected outputs and calculate the error.\n",
    "    3. Calculate the derivatives of the error with respect to the network weights.\n",
    "    4. Adjust the weights to minimize the error.\n",
    "    5. Repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b10b51",
   "metadata": {},
   "source": [
    "**3.\tExplain Vanishing and exploding gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad66170",
   "metadata": {},
   "source": [
    "Vanishing: As the backpropagation algorithm advances downwards(or backward) from the output layer towards the input layer, the gradients often get smaller and smaller and approach zero which eventually leaves the weights of the initial or lower layers nearly unchanged. As a result, the gradient descent never converges to the optimum. This is known as the vanishing gradients problem.\n",
    "\n",
    "Exploding: On the contrary, in some cases, the gradients keep on getting larger and larger as the backpropagation algorithm progresses. This, in turn, causes very large weight updates and causes the gradient descent to diverge. This is known as the exploding gradients problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e620b84",
   "metadata": {},
   "source": [
    "**4.\tExplain Long short-term memory (LSTM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30fed4d",
   "metadata": {},
   "source": [
    "1. Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. The two technical problems overcome by LSTMs are vanishing gradients and exploding gradients\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/1200px-LSTM_Cell.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a62b7",
   "metadata": {},
   "source": [
    "**5.\tExplain Gated recurrent unit (GRU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf644dfe",
   "metadata": {},
   "source": [
    "1. GRU or Gated recurrent unit is an advancement of the standard RNN i.e recurrent neural network.\n",
    "2. GRUs are very similar to Long Short Term Memory(LSTM). Just like LSTM, GRU uses gates to control the flow of information. They are relatively new as compared to LSTM. This is the reason they offer some improvement over LSTM and have simpler architecture.\n",
    "3. GRUs are faster to train because it only has a hidden state. \n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-17-16-32-42.png)\n",
    "\n",
    "4. In GRU we have a two gates they are Reset gate and Update gate.\n",
    "5. In GRU we have only one state i.e, hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3789cfe6",
   "metadata": {},
   "source": [
    "**6.\tExplain Peephole LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90076900",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1400/0*9ofTZTHKBGuMwTxd.png)\n",
    "\n",
    "1. Peephole is the modified version of LSTM. Here we let the gate layers look at the cell state. \n",
    "2. In this peephole connection we can see that all the gates are having an input along with the cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f080a",
   "metadata": {},
   "source": [
    "**7.\tBidirectional RNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ef020",
   "metadata": {},
   "source": [
    "1. Bidirectional recurrent neural networks(RNN) are really just putting two independent RNNs together. The input sequence is fed in normal time order for one network, and in reverse time order for another. The outputs of the two networks are usually concatenated at each time step, though there are other options, e.g. summation.\n",
    "2. This structure allows the networks to have both backward and forward information about the sequence at every time step.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*6QnPUSv_t9BY9Fv8_aLb-Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532c771",
   "metadata": {},
   "source": [
    "**8.\tExplain the gates of LSTM with equations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd54691",
   "metadata": {},
   "source": [
    "There are three different gates in an LSTM cell: a forget gate, an input gate, and an output gate.\n",
    "\n",
    "![](https://pluralsight2.imgix.net/guides/8a8ac7c1-8bac-4e89-ace8-9e28813ab635_3.JPG)\n",
    "\n",
    "**Forget Gate:** The forget gate decides which information needs attention and which can be ignored. The information from the current input X(t) and hidden state h(t-1) are passed through the sigmoid function. Sigmoid generates values between 0 and 1. It concludes whether the part of the old output is necessary (by giving the output closer to 1). \n",
    "\n",
    "![](https://pluralsight2.imgix.net/guides/9af2368b-b0a4-4df7-b952-aab1c2e7ce0f_4.JPG)\n",
    "\n",
    "**Input Gate:** First, the current state X(t) and previously hidden state h(t-1) are passed into the second sigmoid function. The values are transformed between 0 (important) and 1 (not-important). The same information of the hidden state and current state will be passed through the tanh function. To regulate the network, the tanh operator will create a vector (C~(t) ) with all the possible values between -1 and 1.\n",
    "\n",
    "![](https://pluralsight2.imgix.net/guides/e1c3ad7d-ad47-4f3d-aad7-9fb8f3815c06_5.JPG)\n",
    "\n",
    "**Output Gate:** The output gate determines the value of the next hidden state. This state contains information on previous inputs. First, the values of the current state and previous hidden state are passed into the third sigmoid function. Then the new cell state generated from the cell state is passed through the tanh function.\n",
    "\n",
    "![](https://pluralsight2.imgix.net/guides/c1bd2c17-eff9-49d7-86cf-2fb668d677af_7.JPG)\n",
    "\n",
    "To conclude, the forget gate determines which relevant information from the prior steps is needed. The input gate decides what relevant information can be added from the current step, and the output gates finalize the next hidden state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a49ddb",
   "metadata": {},
   "source": [
    "**9.\tExplain BiLSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e1c5f1",
   "metadata": {},
   "source": [
    "1. A Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction.\n",
    "\n",
    "![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_8.54.27_PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10105aac",
   "metadata": {},
   "source": [
    "**10.\tExplain BiGRU**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaccccb",
   "metadata": {},
   "source": [
    "1. A Bidirectional GRU, or BiGRU, is a sequence processing model that consists of two GRUs. one taking the input in a forward direction, and the other in a backwards direction. It is a bidirectional recurrent neural network with only the input and forget gates.\n",
    "\n",
    "![](https://www.researchgate.net/publication/336093319/figure/fig2/AS:807730222735361@1569589300567/The-structure-of-BiGRU.ppm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb5302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
